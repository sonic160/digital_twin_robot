{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exective summary of Work Package 3\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this WP, you will work on a given training dataset. Your goal is to develop a fault detection model using the regression algorithms learnt in the class, in order to achieve best F1 scoreã€‚\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Task 1: Develop a regression model to predict the reference value for motor temperature.\n",
    "- Task 2: Develop a fault detection model using the regression model you developed in Task 1.\n",
    "\n",
    "## Delierables\n",
    "\n",
    "- A Jupyter notebook reporting the process and results of the above tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting, please:\n",
    "- Fetch the most up-to-date version of the github repository.\n",
    "- Create a new branch with your name, based on the \"main\" branch and switch to your own branch.\n",
    "- Copy this notebook to the work space of your group, and rename it to TD_WP_3_Your name.ipynb\n",
    "- After finishing this task, push your changes to the github repository of your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Predict normal behaviors through regression models\n",
    "\n",
    "In this task, let us try to develop a best regression model to predict the normal behaviors of a given motor. In this exercise, we can use motor 6 as an example. You can easilily generate the approach to other models for the data challenge.\n",
    "\n",
    "We can use all the dataset where motor 6 works normally as our dataset. Then, we can run a cross validation (based on sequence, not points) to test the performances of the developed model.\n",
    "\n",
    "In this example, we mainly use the following performance metrics:\n",
    "- max error: The max error between the predicted and the true values.\n",
    "- Mean root squared error: The mean root squared error between the predicted and the true values.\n",
    "- Out-of-boundary rate: The percentage that the residual error between the predicted and the true values is larger than a given threshold. Here, we set the thresold to be $3$ degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all we remove the outliers\n",
    "from utility import read_all_test_data_from_path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pre_processing(df: pd.DataFrame):\n",
    "    ''' # Description\n",
    "    Remove outliers from the dataframe based on defined valid ranges. \n",
    "    Define a valid range of temperature and voltage. \n",
    "    Use ffil function to replace the invalid measurement with the previous value.\n",
    "    '''\n",
    "    df['temperature'] = df['temperature'].where(df['temperature'] <= 100, np.nan)\n",
    "    df['temperature'] = df['temperature'].where(df['temperature'] >= 0, np.nan)\n",
    "    df['temperature'] = df['temperature'].ffill()\n",
    "\n",
    "    df['voltage'] = df['voltage'].where(df['voltage'] >= 6000, np.nan)\n",
    "    df['voltage'] = df['voltage'].where(df['voltage'] <= 9000, np.nan)\n",
    "    df['voltage'] = df['voltage'].ffill()\n",
    "\n",
    "    df['position'] = df['position'].where(df['position'] >= 0, np.nan)\n",
    "    df['position'] = df['position'].where(df['position'] <= 1000, np.nan)\n",
    "    df['position'] = df['position'].ffill()\n",
    "\n",
    "\n",
    "base_dictionary = '../../../dataset/training_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "\n",
    "# Function to design a Butterworth low-pass filter\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "# Function to apply the Butterworth low-pass filter\n",
    "def lowpass_filter(data, cutoff_freq, sampling_freq, order=5):\n",
    "    b, a = butter_lowpass(cutoff_freq, sampling_freq, order=order)\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# Set parameters for the low-pass filter\n",
    "cutoff_frequency = .5  # Adjust as needed\n",
    "sampling_frequency = 10  # Assuming your data is evenly spaced in time\n",
    "\n",
    "\n",
    "def customized_outlier_removal(df: pd.DataFrame):\n",
    "    ''' # Description\n",
    "    Remove outliers from the dataframe based on defined valid ranges. \n",
    "    Define a valid range of temperature and voltage. \n",
    "    Use ffil function to replace the invalid measurement with the previous value.\n",
    "    '''\n",
    "    df['position'] = df['position'].where(df['position'] <= 1000, np.nan)\n",
    "    df['position'] = df['position'].where(df['position'] >= 0, np.nan)\n",
    "    df['position'] = df['position'].ffill()\n",
    "    df['position'] = lowpass_filter(df['position'], cutoff_frequency, sampling_frequency)\n",
    "    # df['position'] = df['position'].rolling(window=10, min_periods=1).mean()\n",
    "    df['position'] = df['position'].round()\n",
    "\n",
    "    df['temperature'] = df['temperature'].where(df['temperature'] <= 100, np.nan)\n",
    "    df['temperature'] = df['temperature'].where(df['temperature'] >= 0, np.nan)\n",
    "    # df['temperature'] = df['temperature'].rolling(window=10, min_periods=1).mean()\n",
    "\n",
    "    # Make sure that the difference between the current and previous temperature cannot be too large.\n",
    "    # Define your threshold\n",
    "    threshold = 10\n",
    "    # Shift the 'temperature' column by one row to get the previous temperature\n",
    "    prev_tmp = df['temperature'].shift(1)\n",
    "    # Calculate the absolute difference between current and previous temperature\n",
    "    temp_diff = np.abs(df['temperature'] - prev_tmp)\n",
    "    # Set the temperature to NaN where the difference is larger than the threshold\n",
    "    df.loc[temp_diff > threshold, 'temperature'] = np.nan\n",
    "    df['temperature'] = df['temperature'].ffill()\n",
    "\n",
    "    df['voltage'] = df['voltage'].where(df['voltage'] >= 6000, np.nan)\n",
    "    df['voltage'] = df['voltage'].where(df['voltage'] <= 8000, np.nan)\n",
    "    df['voltage'] = df['voltage'].ffill()\n",
    "    # df['voltage'] = lowpass_filter(df['voltage'], cutoff_frequency, sampling_frequency)\n",
    "    # df['voltage'] = df['voltage'].rolling(window=5, min_periods=1).mean()\n",
    "    \n",
    "\n",
    "from utility import read_all_csvs_one_test\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get all the folders in the base_dictionary\n",
    "path_list = os.listdir(base_dictionary)\n",
    "# Only keep the folders, not the excel file.\n",
    "path_list = [f for f in path_list if not f.endswith('.xlsx')]\n",
    "\n",
    "# Read the data.\n",
    "df_data_smoothing = pd.DataFrame()\n",
    "for tmp_path in path_list:\n",
    "    path = base_dictionary + tmp_path\n",
    "    # Read the data with the customized outlier removal function.\n",
    "    tmp_df = read_all_csvs_one_test(path, tmp_path, customized_outlier_removal)\n",
    "    df_data_smoothing = pd.concat([df_data_smoothing, tmp_df])\n",
    "    df_data_smoothing = df_data_smoothing.reset_index(drop=True)\n",
    "\n",
    "# Read the test conditions\n",
    "df_test_conditions = pd.read_excel(base_dictionary+'Test conditions.xlsx')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-836590b2b8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m'20240426_141980'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     '20240503_164435']\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_condition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_test_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdf_data_smoothing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data_smoothing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_data_smoothing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_condition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_test_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_data' is not defined"
     ]
    }
   ],
   "source": [
    "normal_test_id = ['20240105_164214', \n",
    "    '20240105_165300', \n",
    "    '20240105_165972', \n",
    "    '20240320_152031', \n",
    "    '20240320_153841', \n",
    "    '20240320_155664', \n",
    "    '20240321_122650', \n",
    "    '20240325_135213', \n",
    "    '20240426_141190', \n",
    "    '20240426_141532', \n",
    "    '20240426_141602', \n",
    "    '20240426_141726', \n",
    "    '20240426_141938', \n",
    "    '20240426_141980', \n",
    "    '20240503_164435']\n",
    "df_data = df_data[df_data['test_condition'].isin(normal_test_id)]\n",
    "df_data_smoothing = df_data_smoothing[df_data_smoothing['test_condition'].isin(normal_test_id)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 1: Only use the features at the current moment.\n",
    "\n",
    "[This notebook](demo_regression_mdl.ipynb) provides a basic demonstration of how to set up the experiment. Let us start by considering only using the features from the current moment. In the notebook, we show a baseline using a simple linear regression with all the features. Could you please try to improve the performance of the model?\n",
    "\n",
    "A few possible directions:\n",
    "- Feature selection?\n",
    "- Smoothing?\n",
    "- Removing sequence-to-sequence variablity? Adding features regarding time dynamics (see the TD for last lecture).\n",
    "- Changing to other regression models? For this, you can try different regression models from [here](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "\n",
    "Put your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of the results - Only using features from the current moment**\n",
    "\n",
    "Please add a table in the end, summarying the results from all the models. Please write a few texts to explain what is the best model you got (including the features and preprocessing you did), its performance, and how could you further improve it.\n",
    "\n",
    "| Model   | Max error | MRSE | Exceed boundary rate |\n",
    "|---------|----------|-----------|--------|\n",
    "| Model 1 |   XX.X%  |   XX.X%   |  XX.X% | \n",
    "| Model 2 |   XX.X%  |   XX.X%   |  XX.X% | \n",
    "| Model 3 |   XX.X%  |   XX.X%   |  XX.X% | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 2: Include the features in the past\n",
    "\n",
    "Now, let's consider using the sliding window approach to include the past in the regression model as well. Please have a look at the demo notebook, run your experiment, and report the best models you could have if you apply the sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of the results - Sliding window**\n",
    "\n",
    "Please add a table in the end, summarying the results from all the models. Please write a few texts to explain what is the best model you got (including the features and preprocessing you did), its performance, and how could you further improve it.\n",
    "\n",
    "| Model   (also report parameters like window_size, sample_step, prediction_lead_time, etc.) | Max error | MRSE | Exceed boundary rate |\n",
    "|---------|----------|-----------|--------|\n",
    "| Model 1 |   XX.X%  |   XX.X%   |  XX.X% | \n",
    "| Model 2 |   XX.X%  |   XX.X%   |  XX.X% | \n",
    "| Model 3 |   XX.X%  |   XX.X%   |  XX.X% | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Fault detection based on regression model\n",
    "\n",
    "In this exercise, we use the dataset that contains the failure of motor 6 to test the fault detection model based on the regression model trained before. \n",
    "\n",
    "[This notebook](demo_FaultDetectReg.ipynb) presents a demonstration of how to use the provided supporting function to develop fault detection model based on the regression model. Please have a look at this notebook, and try to improve the performance of the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the results\n",
    "\n",
    "Please add a table in the end, summarying the results from all the models (including the unsupervised learning models). Please write a few texts to explain what is the best model you got (including key parameters like threshold, window_size, sample_step, prediction_lead_time, etc), its performance, and how could you further improve it.\n",
    "\n",
    "| Model   | Accuracy | Precision | Recall | F1   |\n",
    "|---------|----------|-----------|--------|------|\n",
    "| Model 1 |   XX.X%  |   XX.X%   |  XX.X% | XX.X%|\n",
    "| Model 2 |   XX.X%  |   XX.X%   |  XX.X% | XX.X%|\n",
    "| Model 3 |   XX.X%  |   XX.X%   |  XX.X% | XX.X%|\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
