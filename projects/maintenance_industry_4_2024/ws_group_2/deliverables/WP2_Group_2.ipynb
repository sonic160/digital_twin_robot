{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exective summary of Work Package 2\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this WP, you will work on a given training dataset. Your goal is to develop a fault detection model using the classification algorithms learnt in the class, in order to achieve best F1 score.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Task 1: Develop a fault detection model using the unsupervised learning algorithms learnt in the class, in order to achieve best F1 score.\n",
    "- Task 2: With the help of the supporting script, develop a cross-validation scheme to test the performance of the developed classification algorithms.\n",
    "- Task 3: Develop a fault detection model using the classification algorithms learnt in the class, in order to achieve best F1 score.\n",
    "\n",
    "## Delierables\n",
    "\n",
    "- A Jupyter notebook reporting the process and results of the above tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting, please:\n",
    "- Fetch the most up-to-date version of the github repository.\n",
    "- Create a new branch with your name, based on the \"main\" branch and switch to your own branch.\n",
    "- Copy this notebook to the work space of your group, and rename it to TD_WP_2_Your name.ipynb\n",
    "- After finishing this task, push your changes to the github repository of your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Unsupervised learning approaches\n",
    "\n",
    "## Implement the statistical testing approach for fault detection\n",
    "\n",
    "In this exercise, we interpret the statistical testing approach for fault detection. The basic idea of statistical testing approach is that we fit a multi-dimensitional distribution to the observation data under normal working condition. Then, when a new data point arrives, we design a hypothesis test to see whether the new data point is consistent with the distribution. If the new data point is consistent with the distribution, we can conclude that the fault is not due to the faulty component.\n",
    "\n",
    "The benefit of this approach is that, to design the detection algrothim, we do not need failed data. Also, the computational time is short as all we need is just to compute the pdf and compare it to a threshold.\n",
    "\n",
    "In this exercise, you need to:\n",
    "- Fit a multi-dimensitional distribution to the training dataset (all normal samples).\n",
    "- Design a fault detection algorithm based on the fitted distribution to detect faulty components.\n",
    "\n",
    "The following block defines a few functions that you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def estimateGaussian(X):\n",
    "    '''Given X, this function estimates the parameter of a multivariate Gaussian distribution.'''\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma2 = np.var(X, axis=0)\n",
    "    return mu, sigma2\n",
    "\n",
    "\n",
    "def classify(X, distribution, log_epsilon=-50):\n",
    "    '''Given X, this function classifies each sample in X based on the multivariate Gaussian distribution. \n",
    "       The decision rule is: if the log pdf is less than log_epsilon, we predict 1, as the sample is unlikely to be from the distribution, which represents normal operation.\n",
    "    '''\n",
    "    p = distribution.logpdf(X)\n",
    "    predictions = (p < log_epsilon).astype(int)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the dataset `20240105_164214` as training dataset, as all the samples in this dataset are normal operation. We will use the dataset `20240325_155003` as testing dataset. Let us try to predict the state of motor 1. For this, we first extract the position, temperature and voltage of motor 1 as features (you can change the features if you want). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../supporting_scripts/WP_1')\n",
    "\n",
    "from utility import read_all_csvs_one_test\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Specify path to the dictionary.\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "dictionary_name = '20240105_164214'\n",
    "path = base_dictionary + dictionary_name\n",
    "\n",
    "# Read the data.\n",
    "df_data = read_all_csvs_one_test(path, dictionary_name)\n",
    "\n",
    "# Get the features\n",
    "X_train = df_data[['data_motor_1_position', 'data_motor_1_temperature', 'data_motor_1_voltage']]\n",
    "\n",
    "# We do the same to get the test dataset.\n",
    "dictionary_name = '20240325_155003'\n",
    "path = base_dictionary + dictionary_name\n",
    "\n",
    "# Read the data.\n",
    "df_data_test = read_all_csvs_one_test(path, dictionary_name)\n",
    "\n",
    "# Get the features\n",
    "X_test = df_data[['data_motor_1_position', 'data_motor_1_temperature', 'data_motor_1_voltage']]\n",
    "y_test = df_data['data_motor_1_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please design your algorithm below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32369768142125865\n",
      "log epsilon :  -6657200.0\n",
      "F1 Score :  0.6710182767624021\n"
     ]
    }
   ],
   "source": [
    "### Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "features_to_keep = [\"data_motor_2_temperature\", \"data_motor_3_position\", \"data_motor_1_voltage\", \"data_motor_1_temperature\", \"data_motor_1_position\"]\n",
    "\n",
    "X_train = df_data.loc[:, features_to_keep]\n",
    "X_test = df_data_test.loc[:, features_to_keep]\n",
    "y_test = df_data_test['data_motor_1_label']\n",
    "\n",
    "### Moving average\n",
    "X_train_smooth = X_train.copy()\n",
    "X_test_smooth = X_test.copy()\n",
    "window_size = 10\n",
    "for col in X_train.columns:\n",
    "    X_train_smooth[col] = X_train[col].rolling(window=window_size).mean()\n",
    "    X_test_smooth[col] = X_test[col].rolling(window=window_size).mean()\n",
    "X_train_smooth = X_train_smooth.loc[window_size:, :]\n",
    "X_test_smooth = X_test_smooth.loc[window_size:, :]\n",
    "y_test = y_test.loc[window_size:]\n",
    "\n",
    "### Normaliser\n",
    "scaler = RobustScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train_smooth))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test_smooth))\n",
    "\n",
    "# First, we need to fit a MVN distribution to the normal samples.\n",
    "mu, sigma2 = estimateGaussian(X_train)\n",
    "\n",
    "\n",
    "# Construct a multivariate Gaussian distribution to represent normal operation.\n",
    "distribution = multivariate_normal(mean=mu, cov=np.diag(sigma2), allow_singular=True)\n",
    "\n",
    "# Now, let's try to predict the labels of the test set X_test.\n",
    "max_score = 0\n",
    "k_i = 7480000\n",
    "values_log_epsilon = list(np.arange(-10*k_i, -k_i/10, k_i/100))\n",
    "best_k = 1\n",
    "for k in values_log_epsilon:\n",
    "    y_pred = classify(X_test, distribution, log_epsilon=k)\n",
    "    if f1_score(y_test, y_pred) > max_score:\n",
    "        max_score = f1_score(y_test, y_pred)\n",
    "        best_k = k\n",
    "    \n",
    "# Calculate accuracy of the prediction.\n",
    "accuracy = sum(np.asarray(y_test == y_pred))/np.asarray(y_test == y_pred).shape[0]\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"log epsilon : \", best_k)\n",
    "print(\"F1 Score : \", max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussions:**\n",
    "- Can you please try to improve the performance of this approach?\n",
    "    - For example, by normalizating the data?\n",
    "    - By smoothing the data?\n",
    "    - By reducing feature number?\n",
    "    - etc.\n",
    "- The parameter log_epsilon defines the threshold we use for making classification. What happens if you change it?\n",
    "- Could you discuss how we should get the best value for this parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best value of the log_epsilon parameter, we can try to get an approximate value by hand at first to have an estimation of the range of the optimal value. Then we can iterate over the range to see which value gives the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local outiler factor (LOF)\n",
    "\n",
    "The local outlier factor (LOF) algorithm computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. You can easiliy implement LOF in scikit-learn ([tutorial](https://www.datatechnotes.com/2020/04/anomaly-detection-with-local-outlier-factor-in-python.html)).\n",
    "\n",
    "Please implement local outlier factor (LOF) algorithm on the dataset of `20240325_155003`. You can try first to detect the failure of motor 1 using this model. Please calculate the accuracy score of your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.2932288752098489\n",
      "Number of neighbors  206\n",
      "Accuracy :  0.8071256764882742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "sys.path.append('../../supporting_scripts/WP_1')\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "dictionary_name = '20240325_155003'\n",
    "path = base_dictionary + dictionary_name\n",
    "df_data = read_all_csvs_one_test(path, dictionary_name)\n",
    "\n",
    "features_to_keep = [\"data_motor_1_voltage\", \"data_motor_1_temperature\", \"data_motor_1_position\"]\n",
    "\n",
    "X = df_data.loc[:, features_to_keep]\n",
    "y_test = df_data['data_motor_1_label']\n",
    "\n",
    "best_score = 0\n",
    "best_neighbors = 0\n",
    "best_y = 0\n",
    "\n",
    "for n in range(180, 230):\n",
    "\n",
    "    lof = LocalOutlierFactor(n_neighbors=n) \n",
    "    y_pred = lof.fit_predict(X)\n",
    "\n",
    "\n",
    "    for k in range(len(y_pred)):\n",
    "        if y_pred[k] == -1:\n",
    "            y_pred[k] = 1\n",
    "        else:\n",
    "            y_pred[k] = 0\n",
    "\n",
    "    if f1_score(y_pred, y_test) > best_score:\n",
    "        best_score = f1_score(y_pred, y_test)\n",
    "        best_neighbors = n \n",
    "        best_y = y_pred\n",
    "\n",
    "print(\"F1 score : \",best_score)\n",
    "print(\"Number of neighbors \", best_neighbors)\n",
    "print(\"Accuracy : \", sum(y_pred == y_test)/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Develop a cross validation pipeline to evaluate the performance of the model.\n",
    "\n",
    "The idea of cross validation is to split the data into k subsets and use one of them as the test set and the rest as the training set. The performance of the model is evaluated only on the test dataset, while the model is trained on the training dataset. By doing this, we ensure that the evaluation of the model is independent from the training of the model. Therefore, we can detect if the model is overfitted.\n",
    "\n",
    "## k-fold cross validation\n",
    "\n",
    "Here, we use motor 1 as an example to develop a pipeline for cross validation. Below, you have a script that read the data, extract features and get the labels.\n",
    "\n",
    "1. Use sk-learn to split the data into training and testing sets, using a k-fold cross validation with k=5. (Hint: This is a routine task which can be answered easily by language models like chatgpt. You can try prompt like this: `Generate a code in python to split the data X and y into training and testing sets, using a k-fold cross validation with k=5.`)\n",
    "2. Then, train a basic logistic regression model, without hyper-parameter tuning on the training set, and use the testing set to evaluate the performance of the model (calculate accuracy, precision, recall, and F1 score). \n",
    "3. Finally, train a logistic regression model, but use the entire dataset X and y as training data. Then, use the trained model to predict the labels of the same dataset (X). Compare the results with the previous step, and discuss why we should use cross validation to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with testing on the validation set :  0.9650216229966929\n",
      "Accuracy with testing on the training set :  0.9654339046649919\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../supporting_scripts/WP_1')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from utility import read_all_test_data_from_path\n",
    "import pandas as pd\n",
    "\n",
    "# Specify path to the dictionary.\n",
    "# Define the path to the folder 'collected_data'\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "# Read all the data\n",
    "df_data = read_all_test_data_from_path(base_dictionary, is_plot=False)\n",
    "\n",
    "# Extract the features for motor 1: You should replace the features with the ones you have selected in WP1.\n",
    "X = df_data[['data_motor_1_position', 'data_motor_1_temperature', 'data_motor_1_voltage']]\n",
    "\n",
    "# Get the label\n",
    "y = df_data['data_motor_1_label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "### With validation\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "score_valid = clf.score(x_test, y_test)\n",
    "\n",
    "### Without validation\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "score_without_valid = clf.score(x_train, y_train)\n",
    "\n",
    "print(\"Accuracy with testing on the validation set : \", score_valid)\n",
    "print(\"Accuracy with testing on the training set : \", score_without_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your discussions here:\n",
    "\n",
    "Most of the time, the accuracy is higher when we test our classifier on the training set, which is obvious because we train the model so that it can pe efficient on this set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Develop classification-based fault detection models\n",
    "\n",
    "In this task, you are supposed to experiment different classification-based fault detection models to get best F1 score. Please use the 5-fold cross-validation to calculate the best F1 score. You are free to try different models, whether they are discussed in the class or not. To simply your work, you can use the models existed in [scikit-learn](https://scikit-learn.org/stable/supervised_learning.html).\n",
    "\n",
    "Please report all the models you tried, how to you tune their hyperparameters, and the corresponding F1 score. Please note that if you would like to tune the hyperparameter, you can use the `GridSearchCv` function in scikit-learn, but you should use it only on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxde\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10.0}\n",
      "Ratio de 1 théorique :  0.035277636271013754\n",
      "Ratio de 1 pour les prédictions :  0.3706062149770759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.64      0.77      7575\n",
      "           1       0.06      0.62      0.11       277\n",
      "\n",
      "    accuracy                           0.64      7852\n",
      "   macro avg       0.52      0.63      0.44      7852\n",
      "weighted avg       0.95      0.64      0.75      7852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from utility import read_all_test_data_from_path\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../supporting_scripts/WP_1')\n",
    "\n",
    "### Get all the data\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "df_data = read_all_test_data_from_path(base_dictionary, is_plot=False)\n",
    "\n",
    "X = df_data[['data_motor_1_temperature', 'data_motor_2_temperature', 'data_motor_3_temperature', 'data_motor_6_temperature', 'data_motor_3_position']]\n",
    "y = df_data['data_motor_1_label']\n",
    "\n",
    "\n",
    "#Separate the data between train/validation sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "### Preprocessing steps \n",
    "\n",
    "# Scaling \n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = pd.DataFrame(scaler.fit_transform(x_train))\n",
    "x_test = pd.DataFrame(scaler.transform(x_test))\n",
    "\n",
    "# Smoothing the data\n",
    "X_train_smooth = x_train.copy()\n",
    "X_test_smooth = x_test.copy()\n",
    "window_size = 10\n",
    "for col in x_train.columns:\n",
    "    X_train_smooth[col] = x_train[col].rolling(window=window_size).mean()\n",
    "    X_test_smooth[col] = x_test[col].rolling(window=window_size).mean()\n",
    "X_train_smooth = X_train_smooth.loc[window_size:, :]\n",
    "X_test_smooth = X_test_smooth.loc[window_size:, :]\n",
    "\n",
    "x_train = X_train_smooth.copy()\n",
    "x_test = X_test_smooth.copy()\n",
    "y_train = y_train.iloc[window_size:]\n",
    "y_test = y_test.iloc[window_size:]\n",
    "\n",
    "### Classifier\n",
    "\n",
    "clf = SVC(kernel=\"rbf\", class_weight=\"balanced\") # We change the weight because our dataset is unbalanced\n",
    "grid = {\"C\" : np.linspace(10, 50, 1)} #We want high bvalue of C so our model classify correctly the rows where y = 1\n",
    "\n",
    "clf_cv = GridSearchCV(clf, grid, scoring=f1_score, n_jobs=-1)\n",
    "clf_cv.fit(x_train, y_train)\n",
    "y_pred = clf_cv.predict(x_test)\n",
    "\n",
    "print(clf_cv.best_params_)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      7596\n",
      "           1       0.12      0.16      0.14       256\n",
      "\n",
      "    accuracy                           0.93      7852\n",
      "   macro avg       0.55      0.56      0.55      7852\n",
      "weighted avg       0.94      0.93      0.94      7852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "import random as rd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from utility import read_all_test_data_from_path\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../supporting_scripts/WP_1')\n",
    "\n",
    "### Get all the data\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "df_data = read_all_test_data_from_path(base_dictionary, is_plot=False)\n",
    "\n",
    "features_not_to_keep = ['data_motor_1_label', 'data_motor_2_label', 'data_motor_3_label', 'data_motor_4_label', 'data_motor_5_label', 'data_motor_6_label', \"time\", \"test_condition\"]\n",
    "\n",
    "X = df_data[[col for col in df_data.columns if col not in features_not_to_keep]]\n",
    "y = df_data['data_motor_1_label']\n",
    "\n",
    "\n",
    "#Separate the data between train/validation sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "### Preprocessing steps \n",
    "\n",
    "# Scaling \n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = pd.DataFrame(scaler.fit_transform(x_train), index=y_train.index)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), index=y_test.index)\n",
    "\n",
    "# Smoothing the data\n",
    "X_train_smooth = x_train.copy()\n",
    "X_test_smooth = x_test.copy()\n",
    "window_size = 10\n",
    "for col in x_train.columns:\n",
    "    X_train_smooth[col] = x_train[col].rolling(window=window_size).mean()\n",
    "    X_test_smooth[col] = x_test[col].rolling(window=window_size).mean()\n",
    "X_train_smooth = X_train_smooth.iloc[window_size:, :]\n",
    "X_test_smooth = X_test_smooth.iloc[window_size:, :]\n",
    "\n",
    "x_train = X_train_smooth.copy()\n",
    "x_test = X_test_smooth.copy()\n",
    "\n",
    "y_train = y_train.iloc[window_size:]\n",
    "y_test = y_test.iloc[window_size:]\n",
    "\n",
    "# Here we delete some rows where y = 1 so that the dataset is more balanced\n",
    "n, m = x_train.shape \n",
    "indice_y_1 = list(y_train == 1)\n",
    "proportion_a_garder = 0.1 # This is the share of rows whe are keeping\n",
    "\n",
    "indice_a_garder = indice_y_1\n",
    "for k in range(len(indice_a_garder)):\n",
    "    if indice_a_garder[k] == False:\n",
    "        a = rd.random()\n",
    "        if a < proportion_a_garder:\n",
    "            indice_a_garder[k] = True\n",
    "        \n",
    "x_train = x_train[indice_a_garder]\n",
    "y_train = y_train[indice_a_garder]\n",
    "\n",
    "# Boosting\n",
    "\n",
    "xgb = GradientBoostingClassifier()\n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "y_pred = xgb.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.11      0.20      7560\n",
      "           1       0.04      0.96      0.08       292\n",
      "\n",
      "    accuracy                           0.14      7852\n",
      "   macro avg       0.51      0.53      0.14      7852\n",
      "weighted avg       0.95      0.14      0.20      7852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from utility import read_all_test_data_from_path\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../supporting_scripts/WP_1')\n",
    "\n",
    "### Get all the data\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "df_data = read_all_test_data_from_path(base_dictionary, is_plot=False)\n",
    "\n",
    "features_not_to_keep = ['data_motor_1_label', 'data_motor_2_label', 'data_motor_3_label', 'data_motor_4_label', 'data_motor_5_label', 'data_motor_6_label', \"time\", \"test_condition\"]\n",
    "\n",
    "X = df_data[[col for col in df_data.columns if col not in features_not_to_keep]]\n",
    "y = df_data['data_motor_1_label']\n",
    "\n",
    "\n",
    "#Separate the data between train/validation sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "### Preprocessing steps \n",
    "\n",
    "# Scaling \n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = pd.DataFrame(scaler.fit_transform(x_train), index=y_train.index)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), index=y_test.index)\n",
    "\n",
    "# Smoothing the data\n",
    "X_train_smooth = x_train.copy()\n",
    "X_test_smooth = x_test.copy()\n",
    "window_size = 10\n",
    "for col in x_train.columns:\n",
    "    X_train_smooth[col] = x_train[col].rolling(window=window_size).mean()\n",
    "    X_test_smooth[col] = x_test[col].rolling(window=window_size).mean()\n",
    "X_train_smooth = X_train_smooth.iloc[window_size:, :]\n",
    "X_test_smooth = X_test_smooth.iloc[window_size:, :]\n",
    "\n",
    "x_train = X_train_smooth.copy()\n",
    "x_test = X_test_smooth.copy()\n",
    "\n",
    "y_train = y_train.iloc[window_size:]\n",
    "y_test = y_test.iloc[window_size:]\n",
    "\n",
    "# Random Forest with hyperparameter tuning\n",
    "\n",
    "# Dictionnary for the weights (unbalanced datasets)\n",
    "dico = {0 : 10**(-8.5), 1 : 1}\n",
    "\n",
    "clf = RandomForestClassifier(class_weight=dico)\n",
    "grid = {\"n_estimators\" : [10, 50, 100]}\n",
    "\n",
    "clf_cv = GridSearchCV(clf, grid)\n",
    "clf_cv.fit(x_train, y_train)\n",
    "y_pred = clf_cv.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the results\n",
    "\n",
    "Please add a table in the end, summarying the results from all the models (including the unsupervised learning models). Please write a few texts to explain what is the best model you got, its performance, and how could you further improve it.\n",
    "\n",
    "Please note our classifiers randomly select data each time they are run, thus the following results might differ from a few percentage.\n",
    "\n",
    "| Model   | Accuracy | Precision | Recall | F1   |\n",
    "|---------|----------|-----------|--------|------|\n",
    "| SVM     |   64%    |   6%      |  62%   | 11%  |\n",
    "| XGB     |   93%    |   12%     |  16%   | 14%  |\n",
    "|Rd Forest|   14%    |   4%      |  96%   | 8%   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
