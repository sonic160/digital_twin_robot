{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions `combine_csv` and `load_data` are used to automate and simplify the process of loading and combining data from multiple CSV files located in different directories.\n",
    "\n",
    "In summary:\n",
    "- `combine_csv` : combines CSV files from a specified folder into a single DataFrame\n",
    "- `load_data` : loads data from multiple folders and combines them into a single DataFrame.\n",
    "These functions provide a convenient way to process and combine data from multiple CSV files and folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(folder_path, tmp_path):\n",
    "    \"\"\"\n",
    "    Combine all CSV files in a folder into a single DataFrame.\n",
    "    :param folder_path: Path to the folder containing the CSV files\n",
    "    :param seq_idx: Sequence index\n",
    "    :param label: Label of the sequence (Normal - 0, Abnormal - 1)\n",
    "    :return: A single DataFrame containing all the data from the CSV files\n",
    "    \"\"\"\n",
    "\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    # Create an empty DataFrame to store the combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the CSV files in the folder\n",
    "    for file in csv_files:\n",
    "        # Construct the full path to each CSV file\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Read each CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Drop the time. Will add later.\n",
    "        df = df.drop(labels=df.columns[0], axis=1)\n",
    "\n",
    "        # Extract the file name (excluding the extension) to use as a prefix\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "\n",
    "        # Add a prefix to each column based on the file name\n",
    "        df = df.add_prefix(f'{file_name}_')\n",
    "\n",
    "        # Concatenate the current DataFrame with the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, df], axis=1)\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    combined_df = pd.concat([df['time'], combined_df], axis=1)\n",
    "    combined_df.loc[:, 'test_condition'] = tmp_path\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def load_data(path_header, folder_path):\n",
    "    df = pd.DataFrame()\n",
    "    for tmp_path in folder_path:\n",
    "        # path = path_header + tmp_path\n",
    "        path = path_header + '/' + tmp_path\n",
    "        tmp_df = combine_csv(path, tmp_path)\n",
    "        df = pd.concat([df, tmp_df])\n",
    "        df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to be able to preprocess the data we are now able to read. The following functions allow datasets preparation and augmentation for machine learning models :\n",
    "\n",
    "- `separate_features_and_target_variables`: This function takes a DataFrame (`df`), a list of training feature names (`training_features`), and the name of the target variable (`target_variable`). It separates the features and target variables from the DataFrame and returns them as a tuple.\n",
    "\n",
    "- `delete_outliers`: This function identifies and removes outliers from a DataFrame (`X`) based on the z-scores of its columns. It calculates the z-scores, finds the rows where the z-scores are greater than a threshold value (`threshold`), and deletes those rows from the DataFrame. It also separates the features and target variables from the modified DataFrame and returns them.\n",
    "\n",
    "- `add_1st_derivative_features`: This function calculates the first derivative of each column in a DataFrame (`X`) and adds the derivative features to the DataFrame. It iterates over the columns, calculates the first derivative using the `np.gradient` function, and appends the derivative features to a new DataFrame. Finally, it concatenates the new DataFrame with the original DataFrame and returns the combined DataFrame.\n",
    "\n",
    "- `oversample_minority_class`: This function performs oversampling of the minority class using the Synthetic Minority Over-sampling Technique (SMOTE). It takes the training data (`X_train`) and training labels (`y_train`) as input, creates an SMOTE object, fits it on the training data and labels, and generates synthetic samples to balance the class distribution. It returns the oversampled training data and labels.\n",
    "\n",
    "- `smoothen_data`: This function applies a moving average to smoothen the data in a DataFrame (`X`). It uses the `rolling` method to calculate the moving average with a specified window size (`window_size`). The smoothened data is returned as a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_features_and_target_variables(df, training_features, target_variable):\n",
    "    \"\"\"\n",
    "    Separate the features and target variables.\n",
    "    :param df: The DataFrame containing the features and target variables\n",
    "    :param training_features: The names of the training features\n",
    "    :param target_variable: The name of the target variable\n",
    "    :return: A tuple containing the features DataFrame and the target variable Series\n",
    "    \"\"\"\n",
    "    X = df[training_features]\n",
    "    y = df[target_variable]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def delete_outliers(X, y, threshold=3):\n",
    "    \"\"\"\n",
    "    Delete outliers from the DataFrame.\n",
    "    :param df: The DataFrame containing the features and target variables\n",
    "    :param threshold: The threshold used to identify outliers\n",
    "    :return: The DataFrame with the outliers removed\n",
    "    \"\"\"\n",
    "    print(y.value_counts())\n",
    "\n",
    "    # Merge X and y\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "\n",
    "    # Calculate the z-scores for each column\n",
    "    z_scores = np.abs((df - df.mean()) / df.std())\n",
    "\n",
    "    # Find the rows whose z-scores are greater than the threshold\n",
    "    outlier_indices = np.where(z_scores > threshold)[0]\n",
    "\n",
    "    # Delete the rows whose z-scores are greater than the threshold, with reindexing\n",
    "    df = df.drop(outlier_indices, axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Separate the features and target variables\n",
    "    X = df.drop(y.name, axis=1)\n",
    "    y = df[y.name]\n",
    "    print(y.value_counts())\n",
    "    return X, y\n",
    "\n",
    "def add_1st_derivative_features(X):\n",
    "    \"\"\"\n",
    "    Add derivative features to the training and test data.\n",
    "    :param X: dataframe in pandas format\n",
    "    :return: Training and test data with the derivative features added\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame to store the derivative features\n",
    "    X_new = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the columns in the DataFrame\n",
    "    for col in X.columns:\n",
    "        # Calculate the first derivative of the column\n",
    "        first_derivative = np.gradient(X[col])\n",
    "\n",
    "        # Create a new column name for the first derivative\n",
    "        first_derivative_name = col + '_1st_der'\n",
    "\n",
    "        # Add the first derivative to the new DataFrame\n",
    "        X_new[first_derivative_name] = first_derivative\n",
    "\n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    X_new = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    return X_new\n",
    "\n",
    "def oversample_minority_class(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Oversample the minority class using SMOTE.\n",
    "    :param X_train: Training data\n",
    "    :param y_train: Training labels\n",
    "    :return: Oversampled training data and training labels\n",
    "    \"\"\"\n",
    "    # Create an SMOTE object\n",
    "    sm = SMOTE(random_state=42)\n",
    "\n",
    "    # Fit the SMOTE object to the training data and labels\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "def smoothen_data(X, window_size):\n",
    "    \"\"\"\n",
    "    Smoothen the data using a moving average.\n",
    "    :param X: Training or test data\n",
    "    :param window_size: Window size for the moving average\n",
    "    :return: Smoothened training or test data\n",
    "    \"\"\"\n",
    "    X_smoothen = X.rolling(window=window_size, min_periods=1).mean()\n",
    "    return X_smoothen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've taken the decision to train different models regarding on the motor we're monitoring. It means that in the end, we'll run at least 6 parallel models for failure detection. Each model will be trained to detect the failure of exactly one motor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training and target features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we know that each instance gathers 18 features : 3 (temperature, voltage, position) for each one of the 6 motors. We are not sure whether it is necessary to keep these 18 features in our models. This is why we are building different sets of input features :\n",
    "- `temperature` : All temperature values from the 6 motors (6 features).\n",
    "- `voltage` : All voltage values from the 6 motors (6 features).\n",
    "- `position` : All position values from the 6 motors (6 features).\n",
    "- `solo_motor` : The features relative to the current motor (3 features).\n",
    "- `solo_temperature` : The temperature of the current motor (1 feature).\n",
    "- `all_num` : All features at the same time (18 features).\n",
    "\n",
    "We'll train the models on these 6 different sets of input features.\n",
    "\n",
    "Of course, the target feature of the model will be the `label` of the motor matching the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = dict()\n",
    "training_features[\"temperature\"] = list()\n",
    "training_features[\"voltage\"] = list()\n",
    "training_features[\"position\"] = list()\n",
    "training_features[\"solo_motor\"] = dict()\n",
    "training_features[\"solo_temperature\"] = dict()\n",
    "target_feature = dict()\n",
    "\n",
    "for i in range(1, 7):\n",
    "    training_features[\"temperature\"].append(\"data_motor_{}_temperature\".format(i))\n",
    "    training_features[\"voltage\"].append(\"data_motor_{}_voltage\".format(i))\n",
    "    training_features[\"position\"].append(\"data_motor_{}_position\".format(i))\n",
    "    training_features[\"solo_motor\"][i] = list()\n",
    "    training_features[\"solo_motor\"][i].append(\"data_motor_{}_temperature\".format(i))\n",
    "    training_features[\"solo_motor\"][i].append(\"data_motor_{}_voltage\".format(i))\n",
    "    training_features[\"solo_motor\"][i].append(\"data_motor_{}_position\".format(i))\n",
    "    training_features[\"solo_temperature\"][i] = list()\n",
    "    training_features[\"solo_temperature\"][i].append(\"data_motor_{}_temperature\".format(i))\n",
    "    target_feature[i] = \"data_motor_{}_label\".format(i)\n",
    "\n",
    "training_features[\"all_num\"] = training_features[\"temperature\"] + training_features[\"voltage\"] + training_features[\"position\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and read the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to choose the training and testing dataset among the data we collected on the actual robot. We'll define the testing datasets later in this notebook, but we make sure these two datasets do not overlap : if so, the test results will be overly optimistic, as the model was using the same data during training already.\n",
    "\n",
    "As the selection is done, we read and store the available data in Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data to use for training\n",
    "path_training = [\n",
    "    'motor1_group2', 'motor2_group2', 'motor3_group2',\n",
    "    'motor4_group2', 'motor5_group2', 'motor6_group2',\n",
    "    'static_with_fault_1', 'static_with_fault_2', 'static_with_fault_3', \n",
    "    'static_with_fault_4', 'static_with_fault_5', 'static_with_fault_6',\n",
    "    # 'steady_state_after_movement', 'steady_state_not_moving',\n",
    "]\n",
    "\n",
    "path_header = os.path.abspath('../data_collection/collected_data/')\n",
    "\n",
    "# Load the data\n",
    "df = load_data(path_header, path_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data and find the best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's choose which classifiers we'll train the data on, and which input features we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of classifiers to evaluate\n",
    "classifiers = [\n",
    "    ('LogReg', LogisticRegression(class_weight='balanced', max_iter=1000)),\n",
    "    # ('SVM', SVC(class_weight='balanced')),\n",
    "    # ('Decision Tree', DecisionTreeClassifier(class_weight='balanced')),\n",
    "    # ('Random Forest', RandomForestClassifier(class_weight='balanced')),\n",
    "    # Add more classifiers here\n",
    "]\n",
    "\n",
    "# Define hyperparameters for grid search for each classifier\n",
    "param_grids = [\n",
    "    {'C': np.logspace(-1, 1, 5)},  # Hyperparameters for Logistic Regression\n",
    "    # {'C': np.logspace(-1, 1, 5), 'gamma': np.logspace(-1, 1, 5), 'kernel': ['poly'], 'degree': [2,3]}, # Hyperparameters for SVM\n",
    "    # {'criterion': ['gini', 'entropy'], 'max_depth': [2, 3, 4]},  # Hyperparameters for Decision Tree\n",
    "    # {'n_estimators': [10, 50, 100, 200], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 3, 4]} # Hyperparameters for Random Forest\n",
    "    # Add more hyperparameters for other classifiers here\n",
    "]\n",
    "\n",
    "# Define the values relative to the preprocessing steps\n",
    "window_size = 15\n",
    "\n",
    "# Create the \"models\" directory if it doesn't exist\n",
    "if not os.path.exists(\"models\"):\n",
    "     os.makedirs(\"models\")\n",
    "\n",
    "training_methods = [\"all_num\",\n",
    "                    \"solo_motor\",\n",
    "                    \"temperature\",\n",
    "                    \"solo_temperature\",\n",
    "                    # \"voltage\",\n",
    "                    # \"position\",\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the models we chose on the training dataset.\n",
    "\n",
    "In the process of selecting the best model, we'll not only optimize on the best F1 Score a model can get, but also on the hyperparameters, using a common strategy called grid search. Per se, grid search is a tuning technique that attempts to compute the optimum values of hyperparameters exhaustively.\n",
    "\n",
    "Once the model is trained, we'll validate it using a part of the training dataset the model wasn't trained on. We'll store the obtained results in a Pandas DataFrame called `val_metrics`. We'll look into it right after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the results (evaluation metrics) for each classifier\n",
    "val_metrics = pd.DataFrame(data=[], columns=['Motor', 'Training Features', 'Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for chosen_training_method in training_methods:\n",
    "\n",
    "    # Select the data to use for training and testing\n",
    "    X, y = dict(), dict()\n",
    "    X_train, X_val, y_train, y_val = dict(), dict(), dict(), dict()\n",
    "\n",
    "    for n_motor in range(1, 7):\n",
    "        chosen_target_feature = target_feature[n_motor]\n",
    "        if chosen_training_method == \"solo_motor\" or chosen_training_method == \"solo_temperature\":\n",
    "            chosen_training_features = training_features[chosen_training_method][n_motor]\n",
    "        else:\n",
    "            chosen_training_features = training_features[chosen_training_method]\n",
    "        \n",
    "        X[n_motor], y[n_motor] = separate_features_and_target_variables(df, chosen_training_features, chosen_target_feature)\n",
    "\n",
    "        # Delete outliers\n",
    "        # X[n_motor], y[n_motor] = delete_outliers(X[n_motor], y[n_motor], threshold = 3)\n",
    "        # It won't work because deleting outliers means keeping only the unfailed data\n",
    "        # So we prefer to keep the outliers\n",
    "        \n",
    "        # Smoothen the data\n",
    "        X[n_motor] = smoothen_data(X[n_motor], window_size=window_size)\n",
    "        \n",
    "        # Add derivative features\n",
    "        X[n_motor] = add_1st_derivative_features(X[n_motor])\n",
    "\n",
    "        # Oversample the minority class\n",
    "        X[n_motor], y[n_motor] = oversample_minority_class(X[n_motor], y[n_motor])\n",
    "\n",
    "        # Split the data into training and validating sets\n",
    "        X_train[n_motor], X_val[n_motor], y_train[n_motor], y_val[n_motor] = train_test_split(X[n_motor], y[n_motor], test_size=0.2, random_state=42)\n",
    "\n",
    "        # Iterate over the classifiers and perform grid search\n",
    "        for classifier, param_grid in zip(classifiers, param_grids):\n",
    "\n",
    "            # Rename the param_grid keys with the classifier name\n",
    "            param_grid = {f'{classifier[0]}__{key}': value for key, value in param_grid.items()}\n",
    "\n",
    "            # Create a pipeline with Standardization and the current classifier\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                classifier\n",
    "            ])\n",
    "\n",
    "            # Use GridSearchCV to find the best hyperparameters and fit the pipeline\n",
    "            grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', verbose = 1)\n",
    "            grid_search.fit(X_train[n_motor], y_train[n_motor])\n",
    "\n",
    "            # Use grid_search.predict to make predictions on the testing dataset\n",
    "            y_pred = grid_search.predict(X_val[n_motor])\n",
    "\n",
    "            # Compute evaluation metrics\n",
    "            conf_matrix = confusion_matrix(y_val[n_motor], y_pred)\n",
    "            accuracy = accuracy_score(y_val[n_motor], y_pred)\n",
    "            precision = precision_score(y_val[n_motor], y_pred)\n",
    "            recall = recall_score(y_val[n_motor], y_pred)\n",
    "            f1 = f1_score(y_val[n_motor], y_pred)\n",
    "\n",
    "\n",
    "            # Store the results in a Pandas DataFrame\n",
    "            new_row = pd.DataFrame({\n",
    "                'Motor': n_motor,\n",
    "                'Training Features': chosen_training_method,\n",
    "                'Classifier': classifier[0],\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1 Score': f1\n",
    "            }, index=[0])\n",
    "            val_metrics = pd.concat([val_metrics, new_row], ignore_index=True)\n",
    "\n",
    "            # Save the best model from grid search\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_model_name = f'best_{classifier[0]}_motor_{n_motor}_{chosen_training_method}.model'\n",
    "\n",
    "            # Create the file\n",
    "            file_path = os.path.join(\"models\", best_model_name)\n",
    "            open(file_path, 'w').close()\n",
    "            joblib.dump(best_model, file_path)\n",
    "\n",
    "# Save the validation metrics to a CSV file\n",
    "file_path = os.path.join(\"models\", 'val_metrics.csv')\n",
    "val_metrics.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously stored the `val_metrics` DataFrame in a `csv` file in the same folder as the dumped models. Let's open it and look at the metrics we got for a specific type of classifier and input features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model metrics using all_num method and LogReg classifier, for each motor (VALIDATION DATASET):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Motor</th>\n",
       "      <th>Training Features</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>all_num</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.986133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>all_num</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.995272</td>\n",
       "      <td>0.990676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>all_num</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.993259</td>\n",
       "      <td>0.991158</td>\n",
       "      <td>0.995157</td>\n",
       "      <td>0.993153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>all_num</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.936714</td>\n",
       "      <td>0.922068</td>\n",
       "      <td>0.952191</td>\n",
       "      <td>0.936887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>all_num</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.972647</td>\n",
       "      <td>0.968954</td>\n",
       "      <td>0.975329</td>\n",
       "      <td>0.972131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>all_num</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.963111</td>\n",
       "      <td>0.952458</td>\n",
       "      <td>0.972840</td>\n",
       "      <td>0.962541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Motor Training Features Classifier  Accuracy  Precision    Recall  F1 Score\n",
       "0      1           all_num     LogReg  0.992958   0.986133  1.000000  0.993018\n",
       "1      2           all_num     LogReg  0.995272   0.990676  1.000000  0.995316\n",
       "2      3           all_num     LogReg  0.993259   0.991158  0.995157  0.993153\n",
       "3      4           all_num     LogReg  0.936714   0.922068  0.952191  0.936887\n",
       "4      5           all_num     LogReg  0.972647   0.968954  0.975329  0.972131\n",
       "5      6           all_num     LogReg  0.963111   0.952458  0.972840  0.962541"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\"models\", 'val_metrics.csv')\n",
    "val_metrics = pd.read_csv(file_path)\n",
    "\n",
    "classifier_name = \"LogReg\"\n",
    "training_method = \"all_num\"\n",
    "\n",
    "best_val_metric = val_metrics.loc[(val_metrics['Classifier'] == classifier_name) & (val_metrics['Training Features'] == training_method)]\n",
    "\n",
    "print(f'Best model metrics using {training_method} method and {classifier_name} classifier, for each motor (VALIDATION DATASET):')\n",
    "\n",
    "best_val_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and read the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define on which data we'll test our models. Remember that the training and testing datasets must not overlap, otherwise the test won't be fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data to use for testing\n",
    "path_test = [\n",
    "    'task_fault',\n",
    "    'fault 1', 'fault 2', 'fault 3', 'fault 4', 'fault 5', 'fault 6'\n",
    "]\n",
    "\n",
    "path_header = os.path.abspath('../data_collection/collected_data/')\n",
    "\n",
    "# Load the data\n",
    "df_test = load_data(path_header, path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual test of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load one model at a time, as they are stored in a specific folder. If the model is unavailable, we print a corresponding message to warn the user.\n",
    "As we did with the validation dataset, we store the obtained metrics in a Pandas DataFrame called `test_metrics`, in which we'll look into a few cells further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pd.DataFrame(columns=['Motor', 'Training Features', 'Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "for chosen_training_method in training_methods:\n",
    "    X_test, y_test = dict(), dict()\n",
    "    for n_motor in range(1, 7):\n",
    "        chosen_target_feature = target_feature[n_motor]\n",
    "        if chosen_training_method == \"solo_motor\" or chosen_training_method == \"solo_temperature\":\n",
    "            chosen_training_features = training_features[chosen_training_method][n_motor]\n",
    "        else:\n",
    "            chosen_training_features = training_features[chosen_training_method]\n",
    "        \n",
    "        X_test[n_motor], y_test[n_motor] = separate_features_and_target_variables(df_test, chosen_training_features, chosen_target_feature)\n",
    "        \n",
    "        # Preprocess the test data\n",
    "        X_test[n_motor] = smoothen_data(X_test[n_motor], window_size=window_size)\n",
    "        X_test[n_motor] = add_1st_derivative_features(X_test[n_motor])\n",
    "        \n",
    "        # Iterate over the classifiers\n",
    "        for classifier in classifiers:\n",
    "            classifier_name = classifier[0]\n",
    "            best_model_name = f'best_{classifier_name}_motor_{n_motor}_{chosen_training_method}.model'\n",
    "            file_path = os.path.join(\"models\", best_model_name)\n",
    "            try:\n",
    "                best_model = joblib.load(file_path)\n",
    "\n",
    "                # Make predictions on the test data\n",
    "                y_pred = best_model.predict(X_test[n_motor])\n",
    "            \n",
    "                # Compute evaluation metrics\n",
    "                conf_matrix = confusion_matrix(y_test[n_motor], y_pred)\n",
    "                accuracy = accuracy_score(y_test[n_motor], y_pred)\n",
    "                precision = precision_score(y_test[n_motor], y_pred)\n",
    "                recall = recall_score(y_test[n_motor], y_pred)\n",
    "                f1 = f1_score(y_test[n_motor], y_pred)\n",
    "                \n",
    "                # Create a new row for the test metrics dataframe\n",
    "                new_row = pd.DataFrame({\n",
    "                    'Motor': n_motor,\n",
    "                    'Training Features': chosen_training_method,\n",
    "                    'Classifier': classifier_name,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'Precision': precision,\n",
    "                    'Recall': recall,\n",
    "                    'F1 Score': f1,\n",
    "                }, index = [0])\n",
    "                \n",
    "                # Add the new row to the test metrics dataframe\n",
    "                test_metrics = pd.concat([test_metrics, new_row], ignore_index=True)\n",
    "            except FileNotFoundError:\n",
    "                print(\"No model for {} motor {} using {} method\".format(classifier_name, n_motor, chosen_training_method))\n",
    "\n",
    "# Save the test metrics to a CSV file\n",
    "file_path = os.path.join(\"models\", 'test_metrics.csv')\n",
    "test_metrics.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the testing metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a similar exploration as we did with the validation metrics earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"models\", 'test_metrics.csv')\n",
    "test_metrics = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model metrics using temperature method and LogReg classifier, for each motor (TESTING DATASET):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Motor</th>\n",
       "      <th>Training Features</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>temperature</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.937982</td>\n",
       "      <td>0.645763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>temperature</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.826113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>temperature</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.875668</td>\n",
       "      <td>0.459574</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.340157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>temperature</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.847181</td>\n",
       "      <td>0.456418</td>\n",
       "      <td>0.626087</td>\n",
       "      <td>0.527956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>temperature</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.684273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>temperature</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.844807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Motor Training Features Classifier  Accuracy  Precision    Recall  \\\n",
       "12      1       temperature     LogReg  0.937982   0.645763  1.000000   \n",
       "13      2       temperature     LogReg  0.826113   0.000000  0.000000   \n",
       "14      3       temperature     LogReg  0.875668   0.459574  0.270000   \n",
       "15      4       temperature     LogReg  0.847181   0.456418  0.626087   \n",
       "16      5       temperature     LogReg  0.684273   0.000000  0.000000   \n",
       "17      6       temperature     LogReg  0.844807   0.000000  0.000000   \n",
       "\n",
       "    F1 Score  \n",
       "12  0.784758  \n",
       "13  0.000000  \n",
       "14  0.340157  \n",
       "15  0.527956  \n",
       "16  0.000000  \n",
       "17  0.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_name = \"LogReg\"\n",
    "training_method = \"temperature\"\n",
    "\n",
    "best_test_metric = test_metrics.loc[(test_metrics['Classifier'] == classifier_name) & (test_metrics['Training Features'] == training_method)]\n",
    "\n",
    "print(f'Best model metrics using {training_method} method and {classifier_name} classifier, for each motor (TESTING DATASET):')\n",
    "\n",
    "best_test_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all the models we generated and trained, now is the time to choose the best. In order to achieve this goal, we'll take a look at the metrics of our models on the testing dataset, and choose the model having the best F1 Score for each motor.\n",
    "\n",
    "The output DataFrame summarizes the best models we got, based on the following choices we made:\n",
    "- The available classifiers\n",
    "- Their possible hyperparameters\n",
    "- The available input features\n",
    "- The training dataset\n",
    "- The testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Motor</th>\n",
       "      <th>Training Features</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>temperature</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.937982</td>\n",
       "      <td>0.645763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>solo_motor</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.759644</td>\n",
       "      <td>0.229469</td>\n",
       "      <td>0.162116</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>solo_motor</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.933531</td>\n",
       "      <td>0.723350</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.717884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>solo_motor</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.938576</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.693478</td>\n",
       "      <td>0.755030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>solo_temperature</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.899703</td>\n",
       "      <td>0.583551</td>\n",
       "      <td>0.959227</td>\n",
       "      <td>0.725649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>solo_motor</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.641543</td>\n",
       "      <td>0.252143</td>\n",
       "      <td>0.686770</td>\n",
       "      <td>0.368861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Motor Training Features Classifier  Accuracy  Precision    Recall  F1 Score\n",
       "5     1       temperature     LogReg  0.937982   0.645763  1.000000  0.784758\n",
       "4     2        solo_motor     LogReg  0.759644   0.229469  0.162116  0.190000\n",
       "3     3        solo_motor     LogReg  0.933531   0.723350  0.712500  0.717884\n",
       "2     4        solo_motor     LogReg  0.938576   0.828571  0.693478  0.755030\n",
       "1     5  solo_temperature     LogReg  0.899703   0.583551  0.959227  0.725649\n",
       "0     6        solo_motor     LogReg  0.641543   0.252143  0.686770  0.368861"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models = pd.DataFrame(columns=['Motor', 'Training Features', 'Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "for n_motor in test_metrics[\"Motor\"].unique():\n",
    "    try:\n",
    "        current_best_models = test_metrics.loc[(test_metrics[\"Motor\"] == n_motor)]\n",
    "        current_id_for_f1_max = current_best_models['F1 Score'].idxmax()\n",
    "        current_best_model = test_metrics.loc[current_id_for_f1_max]\n",
    "        current_best_model = pd.DataFrame({\n",
    "            'Motor': current_best_model['Motor'],\n",
    "            'Training Features': current_best_model['Training Features'],\n",
    "            'Classifier': current_best_model['Classifier'],\n",
    "            'Accuracy': current_best_model['Accuracy'],\n",
    "            'Precision': current_best_model['Precision'],\n",
    "            'Recall': current_best_model['Recall'],\n",
    "            'F1 Score': current_best_model['F1 Score']\n",
    "        }, index=[0])\n",
    "        best_models = pd.concat([current_best_model, best_models], ignore_index=True)\n",
    "    except:\n",
    "        print(f\"No model for motor {n_motor}\")\n",
    "\n",
    "# Sort the best models by motor number\n",
    "best_models = best_models.sort_values('Motor')\n",
    "\n",
    "# Save the best models to a CSV file\n",
    "file_path = os.path.join(\"models\", 'best_models.csv')\n",
    "best_models.to_csv(file_path, index=False)\n",
    "\n",
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on unlabelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the unlabelled data for group 1 in two identical folders, stored in \"collected_data\", and respectively named \"testing_data_from_group_1\" and \"testing_data_from_group_1_lab\". In the same way, let's create \"testing_data_from_group_3\" and \"testing_data_from_group_3_lab\" with the CSV files from the third group.\n",
    "\n",
    "Next, we'll read the data from \"testing_data_from_group_1\" or \"testing_data_from_group_3\", preprocess and predict the labels, then change the labels in the \"testing_data_from_group_1_lab\" or \"testing_data_from_group_3_lab\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_group = 1\n",
    "\n",
    "# Select the data to use for testing\n",
    "path_test = [f'testing_data_from_group_{n_group}']\n",
    "\n",
    "# Select the folder to save the labelled data\n",
    "labelled_folder = f'testing_data_from_group_{n_group}_lab'\n",
    "\n",
    "\n",
    "path_header = os.path.abspath('../data_collection/collected_data/')\n",
    "\n",
    "# Load the data\n",
    "df_test = load_data(path_header, path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the algorithm worries about NaN values in the expected label column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns that end with \"label\"\n",
    "label_columns = df_test.filter(regex='label$').columns\n",
    "\n",
    "# Replace the NaN in the selected columns with -1\n",
    "df_test[label_columns] = df_test[label_columns].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of time where motor 1 failed: 0.0030461270670147957\n",
      "Ratio of time where motor 2 failed: 0.18973020017406442\n",
      "Ratio of time where motor 3 failed: 0.6949521322889469\n",
      "Ratio of time where motor 4 failed: 0.4442993907745866\n",
      "Ratio of time where motor 5 failed: 0.17232375979112272\n",
      "Ratio of time where motor 6 failed: 0.2711053089643168\n"
     ]
    }
   ],
   "source": [
    "for n_motor in best_models[\"Motor\"].unique():\n",
    "    chosen_training_method = best_models.loc[best_models[\"Motor\"] == n_motor][\"Training Features\"].values[0]\n",
    "    chosen_classifier = best_models.loc[best_models[\"Motor\"] == n_motor][\"Classifier\"].values[0]\n",
    "    chosen_target_feature = target_feature[n_motor]\n",
    "    if chosen_training_method == \"solo_motor\" or chosen_training_method == \"solo_temperature\":\n",
    "        chosen_training_features = training_features[chosen_training_method][n_motor]\n",
    "    else:\n",
    "        chosen_training_features = training_features[chosen_training_method]\n",
    "    \n",
    "    X_test, y_test = separate_features_and_target_variables(df_test, chosen_training_features, chosen_target_feature)\n",
    "\n",
    "    # Preprocess the test data\n",
    "    X_test = smoothen_data(X_test, window_size=window_size)\n",
    "    X_test = add_1st_derivative_features(X_test)\n",
    "    \n",
    "    # Load the best model for the current motor\n",
    "    best_model_name = f'best_{chosen_classifier}_motor_{n_motor}_{chosen_training_method}.model'\n",
    "    file_path = os.path.join(\"models\", best_model_name)\n",
    "    best_model = joblib.load(file_path)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Print the number of instances where the motor failed over the entire test dataset\n",
    "    print(f'Ratio of time where motor {n_motor} failed: {np.count_nonzero(y_pred) / len(y_pred)}')\n",
    "\n",
    "    # Add the predictions to the csv file\n",
    "    filename = f\"data_motor_{n_motor}.csv\"\n",
    "    file_path = os.path.join(path_header, labelled_folder, filename)\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    current_df['label'] = y_pred\n",
    "    current_df.to_csv(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
