{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exective summary of Work Package 2\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This work package aims at developing a data-driven fault detection algorithm for the motors in the robot. The algorithm should be able to accurately detect faults under different working conditions of the robot, and to display the results in the GUI developed in WP1.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Task 1: Training data generation: Generate training data for your model, considering different working conditions of the robot.\n",
    "- Task 2: Model development.\n",
    "  - Data cleaning, preprocessing\n",
    "  - Investigate different possible model and train the final model\n",
    "- Task 3: GUI design and implementation.\n",
    "- Task 4: Data challenge: Each group will generate a separate dataset, which will be used to construct a final evaluation dataset for the other groups. The performance of your model will be evaluated through this testing dataset.\n",
    "\n",
    "## Delierables\n",
    "\n",
    "- A Jupyter notebook reporting the model development process, including but not limited to:\n",
    "  - Data cleaning, preprocessing\n",
    "  - The models you tried and their performance evaluation through cross validation\n",
    "  - The final model with best performance\n",
    "- A demo software for condition-monitoring and fault detection:\n",
    "  - The software should be able to read the data from the robot and display the results in the GUI.\n",
    "  - The software should be able to detect faults in the robot and display the results in the GUI.\n",
    "  - Record a video with the robot to demonstrate the functionality of your software.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working plan for today's TD\n",
    "\n",
    "Today, we mainly work on task 2. More specifically, we will work on the following:\n",
    "1. Explore the data we collected last week, including:\n",
    "    - Data visualization to explore the patterns of the data\n",
    "    - Use Principle Component Analysis (PCA) or t-SNE to visualize the data, in order to have a first flavor about the difficulty of the problem \n",
    "    - Explore if we have a significant difference between different sequences through visualization and clustering\n",
    "2. Data cleaning and preprocessing\n",
    "    - Missing values and outlier handling\n",
    "    - Feature engineering\n",
    "3. Apply statistical testing approach.\n",
    "4. Apply a simple logistic regression model as a benchmark model for the classification task.\n",
    "\n",
    "Before starting, please:\n",
    "- Fetch the most up-to-date version of the github repository.\n",
    "- Create a new branch called \"WP2_TD_Lect_6_YourName\", based on the branch \"zhiguo_dev_DARR\" and switch to it.\n",
    "- Copy the notebook WP2_DATA-DRIVEN FAULT DETECTION/support/WP2_TD_Lect_6 to WP2_DATA-DRIVEN FAULT DETECTION/TD_6/, and rename it to TD_Lect_6_YourName.ipynb\n",
    "- After finishing this task, push your changes to your github repository.\n",
    "- Submit a pull request to the \"zhiguo_dev\" branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 Implement logistic regression\n",
    "\n",
    "Logistic regression is a simple and powerful machine learning algorithm. It is used for classification problems. It is a binary classifier, meaning that it can distinguish between only two classes. Often, it is used as a \"quick and dirty\" method to create benchmark for a classification problem.\n",
    "\n",
    "In this exercise, we will implement logistic regression in our dataset. You will try:\n",
    "- Use sklearn to implement logistic regression\n",
    "- Hyper-parameter tuning in logistic regression\n",
    "- Use cross-validation to evaluate the performance of your model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Effect of hyper-parameter and normalization\n",
    "\n",
    "Below is a sample code for logistic regression.\n",
    "- Try to change different values of hyper-parameter $C$, and see the impact on the results.\n",
    "- In this code, we did not do any preprocessing on the data. You can try to implement your preprocessing and see the impacts on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import read_data\n",
    "\n",
    "# Root path to the CSV files\n",
    "path_normal = r'../Data collection_20231109/Normal sequence'\n",
    "path_failure = r'../Data collection_20231109/Failure sequence'\n",
    "df = read_data(path_normal, path_failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptv_features = [col for col in df.columns if col.endswith(('position', 'temperature', 'voltage'))]\n",
    "window_median = 10\n",
    "window_mean = 20\n",
    "\n",
    "df_smoothed = df.copy()\n",
    "\n",
    "# Group the dataframe by sequence_idx and apply smoothing to each group separately\n",
    "for seq_idx, group in df_smoothed.groupby('sequence_idx'):\n",
    "    for feature in ptv_features:\n",
    "        group[feature] = group[feature].rolling(window=window_median, min_periods=1).median()\n",
    "        group[feature] = group[feature].rolling(window=window_mean, min_periods=1).mean()\n",
    "    df_smoothed.loc[group.index] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (X) and the target variable (y)\n",
    "X = df_smoothed.drop(['label', 'sequence_idx', 'time'], axis=1)\n",
    "y = df_smoothed['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[337  61]\n",
      " [ 53 268]]\n",
      "Accuracy: 0.8414464534075105\n",
      "Precision: 0.8145896656534954\n",
      "Recall: 0.8348909657320872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "# Train a logistic regression model\n",
    "logreg = LogisticRegression(C=0.1)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use $k$-fold cross validation to evaulate the performance\n",
    "\n",
    "In the previous sample code, we used a single train-test split to evaluate the performance of our model. However, in practice, we want to use a more robust method to evaluate the performance of our model. One common method is to use $k$-fold cross validation. In this task, we will use $k$-fold cross validation to evaluate the performance of our model. Please implement a cross-validation with $k=5$, and calculate the average accuracy, precision, recall and F1 score of the tests.\n",
    "\n",
    "This is a routine task which can be easily sovled by current LLM like chatgpt or Tongyi Lingma. I let you try to generate your code by yourself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.7780250347705147\n",
      "Average precision: 0.7946751584859657\n",
      "Average recall: 0.7788415630620642\n",
      "Average F1 score: 0.7746116140751236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the k-fold cross validation\n",
    "k = 5\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg = LogisticRegression(C=0.1)\n",
    "\n",
    "# Print the average accuracy, precision, recall and F1 score of the tests\n",
    "print(\"Average accuracy:\", cross_val_score(logreg, X, y, cv=k, scoring='accuracy').mean())\n",
    "print(\"Average precision:\", cross_val_score(logreg, X, y, cv=k, scoring='precision_macro').mean())\n",
    "print(\"Average recall:\", cross_val_score(logreg, X, y, cv=k, scoring='recall_macro').mean())\n",
    "print(\"Average F1 score:\", cross_val_score(logreg, X, y, cv=k, scoring='f1_macro').mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Tuning the hyper-parameter of the model\n",
    "\n",
    "We can tune the hyper-parameters of our model by using the GridSearchCV function. A more efficient way is to create a pipeline that includes the hyper-parameter tuning, and prediction. You can try to generate the code using the following prompt:\n",
    "\n",
    "```Prompt\n",
    "Create a pipeline, where the first step is to use GridSearchcv to tune the hyper-parameter C of a logistic regression, and the second step is to use the best parameters to make predictions. \n",
    "\n",
    "Try to implement the generated code on our dataset here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Use the pipeline in cross validation\n",
    "\n",
    "Then, you can use your defined pipeline to automatically tune the hyper-parameter and use the best model for classification. Please try this on the code of Task 2, i.e., use the pipeline in the cross validation. You just need to replace the logistic classifier with the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'logreg__C': 0.7924828983539177, 'logreg__penalty': 'l2'}\n",
      "Average accuracy: 0.7977746870653686\n",
      "Average precision: 0.8034439404762423\n",
      "Average recall: 0.7976034898415187\n",
      "Average F1 score: 0.795627959871716\n",
      "Average AUC score: 0.8409352030295019\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define the hyper-parameters to tune\n",
    "parameters = {\n",
    "    'logreg__C': np.logspace(np.log10(0.01), np.log10(100), num=100),\n",
    "    'logreg__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to tune the hyper-parameters and use the best model for classification\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyper-parameters\n",
    "print(\"Best hyper-parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Use the best model to make predictions and evaluate the performance using cross-validation\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the average accuracy, precision, recall and F1 score of the tests\n",
    "print(\"Average accuracy:\", cross_val_score(best_model, X, y, cv=5, scoring='accuracy').mean())\n",
    "print(\"Average precision:\", cross_val_score(best_model, X, y, cv=5, scoring='precision_macro').mean())\n",
    "print(\"Average recall:\", cross_val_score(best_model, X, y, cv=5, scoring='recall_macro').mean())\n",
    "print(\"Average F1 score:\", cross_val_score(best_model, X, y, cv=5, scoring='f1_macro').mean())\n",
    "print(\"Average AUC score:\", cross_val_score(best_model, X, y, cv=5, scoring='roc_auc').mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
